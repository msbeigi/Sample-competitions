{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Topic Modelling:\n",
        "For the second part of this assignment, you will use Gensim's LDA (Latent Dirichlet Allocation) model to model topics in newsgroup_data. You will first need to finish the code in the cell below by using gensim.models.ldamodel.LdaModel constructor to estimate LDA model parameters on the corpus, and save to the variable ldamodel. Extract 10 topics using corpus and id_map, and with passes=25 and random_state=34."
      ],
      "metadata": {
        "id": "60sdVUfH7r8E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XY0JKod7lMT",
        "outputId": "822c6099-137c-47ab-c0dd-96d8afd2fd50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "# import gensim\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# from gensim.models import LdaModel\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import wordnet as wn\n",
        "import pandas as pd\n",
        "nltk.data.path.append(\"assets/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the corpus in txt file into pickle\n",
        "with open(r'/content/newsgroups.txt', 'r') as f:\n",
        "    newsgroups = f.readlines()\n",
        "\n",
        "# Save the corpus using pickle.dump()\n",
        "with open('newsgroups.pkl', 'wb') as f:\n",
        "    pickle.dump(newsgroups, f)\n",
        "\n"
      ],
      "metadata": {
        "id": "UxLvhUDOCxl2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the corpus content\n",
        "\n",
        "with open(r'newsgroups.pkl', 'rb') as f:\n",
        "    newsgroup_data = pickle.load(f)\n",
        "len(newsgroup_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkBHaOfs7w1V",
        "outputId": "c9bf0119-be85-45e1-e515-32b3168d39ed"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def lda_topics():\n",
        "\n",
        "    # Load the list of documents\n",
        "    with open('newsgroups.pkl', 'rb') as f:\n",
        "        newsgroup_data = pickle.load(f)\n",
        "\n",
        "    # The CountVectorizor to find three letter tokens, remove stop_words, \n",
        "    # removing tokens that don't appear in at least 20 documents,\n",
        "    # removing tokens that appear in more than 20% of the documents\n",
        "    vect = CountVectorizer(min_df=20, max_df=0.2, stop_words='english', \n",
        "                           token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')\n",
        "    # Fit and transform\n",
        "    X = vect.fit_transform(newsgroup_data)\n",
        "\n",
        "    # Convert sparse matrix to gensim corpus.\n",
        "    corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
        "\n",
        "    # Mapping from word IDs to words (To be used in LdaModel's id2word parameter)\n",
        "    id_map = dict((v, k) for k, v in vect.vocabulary_.items())\n",
        "    \n",
        "    ldamodel=LdaModel(corpus=corpus, num_topics=10, id2word=id_map, passes=15, random_state=42)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    topics = ldamodel.print_topics(num_topics=10, num_words=10)\n",
        "    # print(topics)\n",
        "    return topics"
      ],
      "metadata": {
        "id": "fmk2xjcE9aOZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_topics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PdfGaYBDkK8",
        "outputId": "aa7d9192-94fb-4961-8b84-ec3bfd6b9e3d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.039*\"don\" + 0.029*\"know\" + 0.023*\"people\" + 0.018*\"think\" + 0.016*\"work\" + 0.013*\"science\" + 0.013*\"doesn\" + 0.013*\"way\" + 0.013*\"area\" + 0.012*\"look\"'),\n",
              " (1,\n",
              "  '0.025*\"ground\" + 0.020*\"like\" + 0.020*\"people\" + 0.019*\"current\" + 0.017*\"does\" + 0.016*\"card\" + 0.015*\"used\" + 0.014*\"help\" + 0.014*\"use\" + 0.014*\"don\"'),\n",
              " (2,\n",
              "  '0.044*\"drive\" + 0.025*\"just\" + 0.024*\"car\" + 0.022*\"good\" + 0.015*\"hard\" + 0.015*\"problem\" + 0.013*\"know\" + 0.013*\"like\" + 0.012*\"disk\" + 0.011*\"don\"'),\n",
              " (3,\n",
              "  '0.042*\"god\" + 0.028*\"does\" + 0.021*\"believe\" + 0.017*\"nand\" + 0.016*\"nthat\" + 0.015*\"know\" + 0.014*\"nof\" + 0.014*\"posting\" + 0.014*\"true\" + 0.013*\"said\"'),\n",
              " (4,\n",
              "  '0.066*\"space\" + 0.055*\"nasa\" + 0.053*\"data\" + 0.035*\"information\" + 0.035*\"available\" + 0.028*\"program\" + 0.026*\"use\" + 0.017*\"edu\" + 0.015*\"mail\" + 0.014*\"com\"'),\n",
              " (5,\n",
              "  '0.032*\"year\" + 0.030*\"team\" + 0.030*\"think\" + 0.022*\"don\" + 0.019*\"better\" + 0.017*\"play\" + 0.015*\"like\" + 0.015*\"season\" + 0.014*\"just\" + 0.014*\"good\"'),\n",
              " (6,\n",
              "  '0.051*\"bike\" + 0.036*\"time\" + 0.029*\"bus\" + 0.025*\"use\" + 0.018*\"want\" + 0.018*\"know\" + 0.017*\"just\" + 0.016*\"thought\" + 0.015*\"don\" + 0.014*\"car\"'),\n",
              " (7,\n",
              "  '0.061*\"power\" + 0.027*\"like\" + 0.026*\"100\" + 0.022*\"monitor\" + 0.021*\"card\" + 0.021*\"new\" + 0.021*\"went\" + 0.020*\"game\" + 0.018*\"far\" + 0.016*\"numbers\"'),\n",
              " (8,\n",
              "  '0.033*\"just\" + 0.030*\"bit\" + 0.020*\"chip\" + 0.020*\"like\" + 0.018*\"nyou\" + 0.018*\"think\" + 0.017*\"people\" + 0.017*\"board\" + 0.017*\"does\" + 0.016*\"use\"'),\n",
              " (9,\n",
              "  '0.060*\"game\" + 0.047*\"edu\" + 0.025*\"games\" + 0.022*\"second\" + 0.020*\"run\" + 0.019*\"hit\" + 0.018*\"runs\" + 0.017*\"right\" + 0.017*\"win\" + 0.016*\"got\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "topic_names:\n",
        "\n",
        "From the list of the following given topics, assign topic names to the topics you found. If none of these names best matches the topics you found, create a new 1-3 word \"title\" for the topic.\n",
        "\n",
        "Topics: Health, Science, Automobiles, Politics, Government, Travel, Computers & IT, Sports, Business, Society & Lifestyle, Religion, Education.\n",
        "\n",
        "*This function should return a list of 10 strings.*"
      ],
      "metadata": {
        "id": "Tjz8Q-Ct9l3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def topic_names():\n",
        "\n",
        "    # Define list of topic names\n",
        "    list_topics = ['Health', 'Science', 'Automobiles', 'Politics', 'Government', 'Travel', \n",
        "                   'Computers & IT', 'Sports', 'Business', 'Society & Lifestyle', 'Religion', 'Education']\n",
        "\n",
        "    # Load the list of documents\n",
        "    with open('newsgroups.pkl', 'rb') as f:\n",
        "        newsgroup_data = pickle.load(f)\n",
        "\n",
        "    # Use CountVectorizor to find three letter tokens, remove stop_words, \n",
        "    # remove tokens that don't appear in at least 20 documents,\n",
        "    # remove tokens that appear in more than 20% of the documents\n",
        "    vect = CountVectorizer(min_df=20, max_df=0.2, stop_words='english', \n",
        "                           token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')\n",
        "    # Fit and transform\n",
        "    X = vect.fit_transform(newsgroup_data)\n",
        "\n",
        "    # Convert sparse matrix to gensim corpus.\n",
        "    corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
        "\n",
        "\n",
        "\n",
        "    # Mapping from word IDs to words (To be used in LdaModel's id2word parameter)\n",
        "    id_map = dict((v, k) for k, v in vect.vocabulary_.items())\n",
        "\n",
        "    # Train LDA model\n",
        "    ldamodel = LdaModel(corpus=corpus, num_topics=10, id2word=id_map, passes=15, random_state=42)\n",
        "\n",
        "    # Convert to gensim corpus\n",
        "    new_doc_corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
        "\n",
        "    # Get the topic distribution\n",
        "    topic_distribution = ldamodel.get_document_topics(new_doc_corpus)\n",
        "\n",
        "    # Create a list to store the topics found\n",
        "    topics_found = []\n",
        "\n",
        "    # Set a threshold for the probability\n",
        "    threshold = 0.1\n",
        "\n",
        "    # Check each topic probability against the threshold\n",
        "    for topic, prob in topic_distribution[0]:\n",
        "        if prob > threshold:\n",
        "            topics_found.append(list_topics[topic])\n",
        "\n",
        "    # Print the topics found\n",
        "    # print(topics_found)\n",
        "    # raise NotImplementedError()\n",
        "    return topics_found"
      ],
      "metadata": {
        "id": "_Sdo6a0i9cIl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_names()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18nsV0m8Dpqb",
        "outputId": "7fc633c1-ac7d-499f-c62c-c0a6e4730d6b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Health', 'Government']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "topic_distribution:\n",
        "\n",
        "For the new document `new_doc`, find the topic distribution. Remember to use vect.transform on the the new doc, and Sparse2Corpus to convert the sparse matrix to gensim corpus.\n",
        "\n",
        "*This function should return a list of tuples, where each tuple is `(#topic, probability)`*"
      ],
      "metadata": {
        "id": "DuEZr41z9rgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_doc = [\"\\n\\nIt's my understanding that the freezing will start to occur because \\\n",
        "of the\\ngrowing distance of Pluto and Charon from the Sun, due to it's\\nelliptical orbit. \\\n",
        "It is not due to shadowing effects. \\n\\n\\nPluto can shadow Charon, and vice-versa.\\n\\nGeorge \\\n",
        "Krumins\\n-- \"]"
      ],
      "metadata": {
        "id": "2Q8rK7CV9iCP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def topic_distribution():\n",
        "    vect = CountVectorizer( stop_words='english', \n",
        "                       token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')\n",
        "    # Fit and transform\n",
        "    X = vect.fit_transform(new_doc)\n",
        "    # Convert sparse matrix to gensim corpus.\n",
        "    corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
        "\n",
        "    # Mapping from word IDs to words (To be used in LdaModel's id2word parameter)\n",
        "    id_map = dict((v, k) for k, v in vect.vocabulary_.items())\n",
        "\n",
        "    \n",
        "    new_doc_transformed = vect.transform(new_doc)\n",
        "    \n",
        "    # Train LDA model\n",
        "    ldamodel = LdaModel(corpus=corpus, num_topics=10, id2word=id_map, passes=15, random_state=42)\n",
        "\n",
        "    # Convert to gensim corpus\n",
        "    new_doc_corpus = gensim.matutils.Sparse2Corpus(new_doc_transformed, documents_columns=False)\n",
        "\n",
        "    # Get the topic distribution\n",
        "    topic_distribution = ldamodel.get_document_topics(new_doc_corpus)\n",
        "\n",
        "    # Return list of tuples (#topic, probability)\n",
        "    topic_distribution = [( topic, prob) for topic, prob in topic_distribution[0]]\n",
        "    \n",
        "    return topic_distribution"
      ],
      "metadata": {
        "id": "wM-MHO0R9wDX"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_distribution()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh8Ns1jj9zOt",
        "outputId": "eedc9cf1-8a23-4db8-a9f9-c9d71c7e4265"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 0.9571427)]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ab5bYwfQEVjh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}